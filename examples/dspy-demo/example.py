import os
import sys
sys.path.append(os.getcwd())

from functools import partial
import dspy
from dotenv import load_dotenv
from dspy.datasets import HotPotQA
from dspy.evaluate import Evaluate
from dspy.teleprompt import BootstrapFewShot
from sentence_transformers import SentenceTransformer
from tidb_vector.integrations import TiDBVectorClient
from utils import sentence_transformer_embedding_function, TidbRM, RAG

# Load the environment variables from the .env file.
load_dotenv()
print("SENTENCE_TRANSFORMERS_MODEL:{}".format(os.environ.get("SENTENCE_TRANSFORMERS_MODEL")))
print("TIDB_DATABASE_URL:{}".format(os.environ.get("TIDB_DATABASE_URL")))
print("LM_MODEL_NAME:{}".format(os.environ.get("LM_MODEL_NAME")))
print("OLLAMA_BASE_URL:{}".format(os.environ.get("OLLAMA_BASE_URL")))

embed_model = SentenceTransformer(os.environ.get('SENTENCE_TRANSFORMERS_MODEL'), trust_remote_code=True)
embed_model_dim = embed_model.get_sentence_embedding_dimension()
embedding_function = partial(sentence_transformer_embedding_function, embed_model)

# The configuration for the TiDBVectorClient.
tidb_vector_client = TiDBVectorClient(
    # The table which will store the TiDB vector data.
    table_name=os.environ.get('TIDB_TABLE_NAME', 'embedded_documents'),
    # The connection string to the TiDB cluster.
    # The connection string should be in the format of:
    # mysql+pymysql://<USER>:<PASSWORD>@<HOST>:4000/<DATABASE>?ssl_ca=<CA_PATH>&ssl_verify_cert=true&ssl_verify_identity=true
    connection_string=os.environ.get('TIDB_DATABASE_URL'),
    # The dimension of the vector generated by the embedding model.
    vector_dimension=embed_model_dim,
    # Determine whether to recreate the table if it already exists.
    drop_existing_table=True,
)

print("Connected to TiDB.")
print("describe table:", tidb_vector_client.execute("describe embedded_documents;"))

print("Initializing the TidbRM model...")
retriever_model = TidbRM(tidb_vector_client=tidb_vector_client, embedding_function=embedding_function)
print("TidbRM model initialized successfully.")

print("Loading sample data...")
# test sample data
# load sample_data.txt  if not local file, you can use requests.get(url).text
# sample data url: https://raw.githubusercontent.com/wxywb/dspy_dataset_sample/master/sample_data.txt
with open('sample_data.txt', 'r') as f:
    # I prepare a small set of data for speeding up embedding, you can replace it with your own data.
    print("sample_data.txt found.")
    sample_data = f.read()
print("Sample data loaded successfully.")
print("Debug sample_data:{}".format(sample_data))

print("Embedding sample data...")
documents = []
for idx, passage in enumerate(sample_data.split('\n')):
    embedding = embedding_function([passage])[0]
    print(idx, passage[:10], embedding[:5])
    if len(passage) == 0:
        continue
    documents.append({
        "id": str(idx),
        "text": passage,
        "embedding": embedding,
        "metadata": {"category": "album"},
    })
print("Sample data embedded successfully.")
print("Sample data number:", len(documents))

print("Inserting documents into TiDB...")
tidb_vector_client.insert(
    ids=[doc["id"] for doc in documents],
    texts=[doc["text"] for doc in documents],
    embeddings=[doc["embedding"] for doc in documents],
    metadatas=[doc["metadata"] for doc in documents],
)
print("Documents inserted successfully.")

language_model = dspy.OllamaLocal(
    model=os.environ.get('LM_MODEL_NAME', 'llama3:8b'),
    base_url=os.environ.get('OLLAMA_BASE_URL'),
    api_key=os.environ.get('OLLAMA_API_KEY')
)
dspy.settings.configure(lm=language_model)

rag = RAG(retriever_model)

dataset = HotPotQA(train_seed=1, train_size=2, eval_seed=2023, dev_size=5, test_size=0)
# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.
trainset = [x.with_inputs('question') for x in dataset.train]
devset = [x.with_inputs('question') for x in dataset.dev]

metric = dspy.evaluate.answer_exact_match
evaluate_on_hotpotqa = Evaluate(devset=devset[:], display_progress=True, display_table=False)
score = evaluate_on_hotpotqa(rag, metric=metric)
print('rag:', score)


# Validation logic: check that the predicted answer is correct.
# Also check that the retrieved context does contain that answer.
def validate_context_and_answer(example, pred, trace=None):
    answer_em = dspy.evaluate.answer_exact_match(example, pred)
    answer_pm = dspy.evaluate.answer_passage_match(example, pred)
    return answer_em and answer_pm


# Set up a basic teleprompter, which will compile our RAG program.
teleprompter = BootstrapFewShot(metric=validate_context_and_answer)

# Compile!
compiled_rag = teleprompter.compile(rag, trainset=trainset)
# Now compiled_rag is optimized and ready to answer your new question!
score = evaluate_on_hotpotqa(compiled_rag, metric=metric)
print('compile_rag:', score)

if __name__ == '__main__':
    print("Answering the question: 'who write At My Window'...")
    print(rag("who write At My Window").answer)
    print(language_model.inspect_history(n=1))
